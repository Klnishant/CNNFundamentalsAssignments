{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f34129-ecb7-446e-b22c-5701daf93bb1",
   "metadata": {},
   "source": [
    "## 1. Difference between Object Detection and Object Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fede868-a55a-459f-aca3-b292b9846fad",
   "metadata": {},
   "source": [
    "### a. Explain the difference between object detection and object classification in the context of computer vision tasks. Provide examples to illustrate each concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b0d9f0-b69d-46da-8e76-97e3129daa71",
   "metadata": {},
   "source": [
    "Ans--> **Object Classification:**\n",
    "Object classification is a computer vision task that involves assigning a single label or class to an input image to identify the main object present in the image. The goal is to determine what object is present in the image from a predefined set of categories. In object classification, the model's output is a single prediction indicating the most likely class for the entire image.\n",
    "\n",
    "Example: Classifying images of animals into categories like \"cat,\" \"dog,\" \"elephant,\" etc. Given an image of a cat, the classification model should predict the label \"cat.\"\n",
    "\n",
    "**Object Detection:**\n",
    "Object detection is a more complex computer vision task that goes beyond classification. It involves identifying and localizing multiple objects within an image, along with assigning a class label to each detected object. Object detection provides both the category of the object and its spatial location within the image. This task is typically solved by drawing bounding boxes around detected objects and associating class labels with those boxes.\n",
    "\n",
    "Example: Detecting and localizing cars, pedestrians, and traffic signs in a street scene. For instance, in an image of a city street, an object detection model would draw bounding boxes around each car, pedestrian, and traffic sign present in the scene, and label each box accordingly.\n",
    "\n",
    "In summary, the main difference between object classification and object detection lies in the complexity of the task and the level of detail provided by the output. Classification focuses on identifying the main object category in an image, while detection involves identifying and localizing multiple objects with bounding boxes and class labels. Detection is a more challenging task but is crucial for various real-world applications, including autonomous driving, surveillance, and image-based search engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898292-7da2-4c27-97b0-2b3a1fbb1394",
   "metadata": {},
   "source": [
    "## 2. Scenarios where Object Detection is used:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca194783-f308-4261-a93c-8a24b7778f8f",
   "metadata": {},
   "source": [
    "### a. Describe at least three scenarios or real-world applications where object detection techniques are commonly used. Explain the significance of object detection in these scenarios and how it benefits the respective applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681df0b-aa67-407e-8477-7a58fa4ac630",
   "metadata": {},
   "source": [
    "Ans--> Object detection techniques are widely used in various real-world applications where the ability to identify and locate objects within images or video frames is essential. Here are three scenarios where object detection plays a significant role:\n",
    "\n",
    "1. **Autonomous Driving:**\n",
    "   In autonomous driving systems, object detection is crucial for ensuring the safety of passengers, pedestrians, and other vehicles on the road. Object detection algorithms are used to identify and localize objects such as pedestrians, cyclists, vehicles, traffic signs, and traffic lights. By accurately detecting and tracking these objects, self-driving cars can make informed decisions, such as adjusting speed, changing lanes, and applying brakes, to navigate safely through complex traffic scenarios. Object detection enables the car's perception system to understand the environment and respond appropriately, reducing the risk of accidents and collisions.\n",
    "\n",
    "2. **Surveillance and Security:**\n",
    "   Object detection is a fundamental component of surveillance and security systems. These systems use object detection to monitor areas of interest and identify potentially suspicious or unauthorized activities. Examples include detecting intruders in restricted areas, recognizing abandoned bags or objects, and tracking individuals' movements. By using object detection, security personnel can receive real-time alerts and take appropriate actions to prevent security breaches, theft, and other unwanted incidents. Object detection enhances the efficiency and effectiveness of surveillance systems by automating the process of monitoring and analysis.\n",
    "\n",
    "3. **Retail Analytics and Inventory Management:**\n",
    "   Object detection is used in retail environments to track and manage inventory, optimize shelf placement, and enhance customer experiences. Retailers can deploy object detection algorithms to monitor shelves and automatically detect when items are out of stock or incorrectly placed. Additionally, object detection can be applied to analyze customer behavior, such as counting the number of customers in a store, estimating foot traffic in different areas, and identifying product interactions. By gaining insights into customer behavior and inventory status, retailers can improve store layouts, restock items efficiently, and personalize customer interactions, ultimately increasing sales and customer satisfaction.\n",
    "\n",
    "In all of these scenarios, object detection provides a crucial capability that goes beyond simple object classification. It allows systems to accurately identify, locate, and track objects in complex visual environments, enabling applications to make informed decisions and automate processes that would be otherwise time-consuming or error-prone. Object detection technology enhances safety, security, and efficiency across a wide range of industries and applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2c901-90d3-44ea-8e86-4651754b2120",
   "metadata": {},
   "source": [
    "## 3. Image Data as Structured Data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4dac41-1a97-431b-9859-88b96fc8b3c1",
   "metadata": {},
   "source": [
    "### a. Discuss whether image data can be considered a structured form of data. Provide reasoning and examples to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f42f3-fef6-44e6-a923-1269e93f6ec7",
   "metadata": {},
   "source": [
    "Ans--> Image data is generally considered unstructured data rather than structured data. Structured data is characterized by its organized format, where each data point is stored in predefined columns and rows, often within a database or a spreadsheet. On the other hand, unstructured data lacks a fixed format and is more challenging to organize due to its varied nature.\n",
    "\n",
    "Here's why image data is considered unstructured:\n",
    "\n",
    "1. **Lack of Fixed Structure:**\n",
    "   Image data does not adhere to a rigid structure of rows and columns. Instead, it comprises pixel values that represent color or intensity information for each point in the image. Each image can have a different size, making it unsuitable for a structured tabular format.\n",
    "\n",
    "2. **Varied Dimensions:**\n",
    "   Images come in various dimensions (width, height, and channels), such as grayscale (1 channel) or color (3 channels for RGB). Structured data, in contrast, has consistent dimensions for each data point.\n",
    "\n",
    "3. **Rich Content:**\n",
    "   Image data contains rich visual content that is not easily represented in structured tables. Structured data often includes numerical or categorical values that can be easily organized and analyzed, whereas images contain complex patterns, textures, and features that require specialized methods for interpretation.\n",
    "\n",
    "4. **Complex Relationships:**\n",
    "   In structured data, relationships between data points are often well-defined using keys or indices. In image data, relationships between pixels are spatial and context-dependent, making the interpretation of these relationships more complex.\n",
    "\n",
    "Examples to support this distinction:\n",
    "\n",
    "- **Structured Data Example:**\n",
    "   Consider a table of sales transactions with columns like \"Product ID,\" \"Date,\" \"Quantity,\" and \"Price.\" Each row follows a consistent structure, and you can perform standard SQL queries or statistical analyses on this data.\n",
    "\n",
    "- **Image Data Example:**\n",
    "   An image of a cat doesn't have fixed columns or rows. Instead, it consists of a grid of pixel values representing colors at various locations. Each pixel contributes to the overall appearance of the image, and relationships between pixels create complex visual patterns.\n",
    "\n",
    "While image data can be processed and analyzed using deep learning techniques and convolutional neural networks (CNNs), these methods are specifically designed to handle the unstructured nature of images. Transforming image data into a structured format for analysis would involve techniques like feature extraction, which involves extracting meaningful features from images and representing them as structured attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220b08e-1271-4ec5-8537-73778f4e32fd",
   "metadata": {},
   "source": [
    "## 4. Explainig Information in an Imagn for CNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b54a687-3f0c-4ffb-bfcb-73144998b00a",
   "metadata": {},
   "source": [
    "### a. Explain how Convolutional Neural Networks (CNN) can extract and understand information from an image. Discuss the key components and processes involved in analyzing image data using CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154165d4-35cd-491c-82a2-c5385f7ba628",
   "metadata": {},
   "source": [
    "Ans--> Convolutional Neural Networks (CNNs) are a specialized type of deep neural network designed for processing and analyzing image data. They excel at automatically learning and extracting hierarchical features from images, making them well-suited for various computer vision tasks. CNNs have several key components and processes that enable them to understand information from images:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   The core of CNNs is the convolutional layer. A convolutional layer applies a set of learnable filters (also called kernels) to the input image. These filters slide over the image's spatial dimensions and perform element-wise multiplications followed by summation. This operation highlights certain patterns or features, such as edges, corners, or textures. Each filter in a convolutional layer is designed to detect a specific type of feature.\n",
    "\n",
    "2. **Activation Functions (ReLU):**\n",
    "   After the element-wise multiplication and summation, a non-linear activation function like Rectified Linear Unit (ReLU) is applied to introduce non-linearity to the network. ReLU is commonly used in CNNs to ensure that the network can capture complex relationships between features.\n",
    "\n",
    "3. **Pooling Layers:**\n",
    "   Pooling layers perform downsampling by selecting a representative value within a small region of the feature map. The most common pooling operation is max-pooling, which selects the maximum value within each region. Pooling helps reduce the spatial dimensions of the feature maps, making the network more computationally efficient and robust to small variations in object position.\n",
    "\n",
    "4. **Strides and Padding:**\n",
    "   Convolutional and pooling operations use strides and padding to control the size of the output feature maps. Strides determine how much the filter moves during convolution or pooling, affecting the downsampling rate. Padding adds additional pixels around the input to control the size of the output feature maps, maintaining spatial dimensions and mitigating information loss.\n",
    "\n",
    "5. **Multiple Layers and Hierarchical Features:**\n",
    "   CNNs stack multiple convolutional and pooling layers to learn increasingly complex and abstract features. Lower layers capture simple features like edges, while deeper layers capture more complex features like object parts. This hierarchy of features allows CNNs to understand the composition and structure of objects in images.\n",
    "\n",
    "6. **Fully Connected Layers:**\n",
    "   After convolutional and pooling layers, CNNs often include fully connected layers (also known as dense layers). These layers process the high-level features learned by the previous layers and perform classification or regression tasks. Fully connected layers capture global relationships and patterns in the extracted features.\n",
    "\n",
    "7. **Backpropagation and Optimization:**\n",
    "   CNNs are trained using backpropagation, a process that adjusts the weights of the network's layers to minimize a loss function. Optimizers like stochastic gradient descent (SGD) update the weights based on gradients calculated during backpropagation, enabling the network to learn to recognize features relevant to the given task.\n",
    "\n",
    "By processing an image through these layers and components, CNNs are able to learn and extract relevant features from different spatial scales, enabling them to understand and classify objects, scenes, and patterns present in the image. This hierarchical feature extraction is a key reason why CNNs have achieved impressive results in various computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcac3bd-7f70-4691-9df5-df6b4c87afe5",
   "metadata": {},
   "source": [
    "## 5. Flattening Images for ANN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2eef8cb-e9db-4050-8062-c6384ca6e0c1",
   "metadata": {},
   "source": [
    "### a. Discuss why it is not recommended to flatten images directly and input them into an Artificial Neural Network (ANN) for image classification. Highlight the limitations and challenges associated with this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27e8645-cd07-4c4c-adc2-422d624d6875",
   "metadata": {},
   "source": [
    "Ans--> Flattening images and inputting them directly into an Artificial Neural Network (ANN) for image classification is not recommended due to several limitations and challenges associated with this approach. Here's why:\n",
    "\n",
    "1. **Loss of Spatial Information:**\n",
    "   Flattening an image collapses its 2D or 3D structure into a 1D vector. This results in the loss of spatial information, which is critical for understanding the relationships between pixels and capturing local patterns. Images are characterized by spatial arrangements of features, edges, and textures, which are essential for accurate classification. Flattening discards this spatial context, making it challenging for the network to identify meaningful patterns.\n",
    "\n",
    "2. **High Dimensionality:**\n",
    "   Flattening large images, especially those with high-resolution or multiple color channels, results in very high-dimensional input vectors. This can lead to an explosion in the number of parameters in the subsequent layers of the network. The large number of parameters increases computational complexity, memory usage, and training time, making the network harder to train effectively.\n",
    "\n",
    "3. **Inefficient Feature Learning:**\n",
    "   ANNs are designed to learn hierarchical features from structured data, but images are inherently unstructured. Flattening images removes the ability of the network to learn hierarchical features in a meaningful way. ANNs lack the specialized layers and operations, such as convolutions and pooling, that are designed to exploit the spatial hierarchies present in image data.\n",
    "\n",
    "4. **Limited Robustness to Variations:**\n",
    "   Flattening images does not account for translation, rotation, or scaling variations that are commonly present in real-world images. ANNs, when directly fed flattened images, struggle to generalize well to these variations. In contrast, convolutional neural networks (CNNs) are designed to handle these variations through spatial hierarchies learned by convolutional and pooling layers.\n",
    "\n",
    "5. **Increased Sensitivity to Noise:**\n",
    "   Flattened image representations tend to be more sensitive to noise and small perturbations. The network might make incorrect predictions due to the lack of spatial context to distinguish noise from meaningful features.\n",
    "\n",
    "6. **Limited Performance on Complex Data:**\n",
    "   Flattened image representations are inadequate for handling complex images with multiple objects, occlusions, and intricate structures. ANN-based models lack the specialized mechanisms to detect and differentiate between such objects and their relationships.\n",
    "\n",
    "To address these challenges, convolutional neural networks (CNNs) were developed. CNNs are designed specifically to process and understand image data. They use convolutional and pooling layers to capture hierarchical features while preserving spatial relationships, making them more effective for image classification tasks. CNNs have revolutionized the field of computer vision and are the preferred choice for various image-related tasks due to their ability to exploit spatial hierarchies and patterns present in images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7e52d-f635-4e43-9c13-b116b9f7eaa8",
   "metadata": {},
   "source": [
    "## Applying CNN to the MNIST Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c41c32-1be6-4c2e-806c-5c0abe460d90",
   "metadata": {},
   "source": [
    "### a. Explain why it is not necessary to apply CNN to the MNIST dataset for image classification. Discuss the characteristics of the MNIST dataset and how it aligns with the requirements of CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4d1d3-30e9-47ef-8304-6d861606f3e2",
   "metadata": {},
   "source": [
    "Ans--> The MNIST dataset is a classic benchmark dataset commonly used for image classification tasks. It consists of grayscale images of handwritten digits (0 to 9) with a resolution of 28x28 pixels. Each image is labeled with the corresponding digit it represents. While the MNIST dataset has been widely used to evaluate various machine learning algorithms, including convolutional neural networks (CNNs), it is often considered too simple for CNNs to showcase their full capabilities. Here's why applying CNNs to the MNIST dataset is not necessary:\n",
    "\n",
    "1. **Simplicity of the Dataset:**\n",
    "   The MNIST dataset contains relatively small and simple grayscale images of isolated digits. The images lack the complexity, variability, and real-world features present in more challenging datasets like CIFAR-10 or ImageNet. As a result, simpler machine learning models, such as traditional feedforward neural networks or even logistic regression, can achieve high accuracy on MNIST without the need for CNNs.\n",
    "\n",
    "2. **Lack of Spatial Hierarchies:**\n",
    "   CNNs excel at capturing spatial hierarchies and complex patterns within images. However, the MNIST dataset does not exhibit the same level of spatial hierarchies as more complex images. The digits are centered, relatively consistent in size, and do not contain intricate features like textures or object interactions. CNNs are designed to learn from images with richer spatial structures.\n",
    "\n",
    "3. **CNNs are Overkill:**\n",
    "   Applying CNNs to the MNIST dataset might be considered overkill because the dataset's simplicity can be effectively handled by simpler models. CNNs are most valuable when dealing with larger, more diverse datasets where they can automatically learn hierarchical features without the need for hand-engineered feature extraction.\n",
    "\n",
    "4. **Risk of Overfitting:**\n",
    "   Using CNNs on datasets with limited variability, like MNIST, can increase the risk of overfitting. CNNs are powerful models that can capture even small variations in the data, which might not generalize well to new, unseen examples.\n",
    "\n",
    "Despite these points, it's important to note that experimenting with CNNs on MNIST can still provide valuable insights into their behavior and performance on a simpler dataset. However, the true strength of CNNs shines when applied to more complex image datasets that have diverse objects, varying backgrounds, and intricate spatial hierarchies. For such datasets, CNNs are better able to leverage their architecture's capabilities to capture and understand features at different scales and levels of abstraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb391050-8f24-4a37-b1d0-ecd0625b83e1",
   "metadata": {},
   "source": [
    "## Extracting Features at Local Space:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7276b8e2-0b1f-4cbd-9f9e-6372fcc3d4f5",
   "metadata": {},
   "source": [
    "### a. Justify why it is important to extract features from an image at the local level rather than considering the entire image as a whole. Discuss the advantages and insights gained by performing local feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d165b-0213-4114-be41-63813193b0ca",
   "metadata": {},
   "source": [
    "Ans--> Extracting features from an image at the local level, rather than considering the entire image as a whole, is essential for various reasons, primarily related to capturing the rich details, patterns, and contextual information present in the image. Local feature extraction provides several advantages and insights that are crucial for tasks like image analysis, object recognition, and computer vision in general:\n",
    "\n",
    "1. **Hierarchical Patterns:**\n",
    "   Images contain hierarchical patterns that can vary significantly from one region to another. By analyzing local patches or regions within an image, local feature extraction allows us to capture intricate patterns, textures, edges, and object parts that might not be as pronounced at the global image level. This enables the identification of fine-grained details that contribute to understanding the content of the image.\n",
    "\n",
    "2. **Object Localization:**\n",
    "   Many computer vision tasks, such as object detection, require precise localization of objects within an image. By extracting local features, we can identify regions of interest that correspond to potential objects or regions with specific attributes. This information is critical for tasks where object boundaries or positions need to be accurately determined.\n",
    "\n",
    "3. **Contextual Understanding:**\n",
    "   Local feature extraction takes into account the surrounding context of each region. Objects often appear in specific contexts or arrangements. Analyzing local features helps in capturing the contextual cues that aid in understanding relationships between different parts of the image.\n",
    "\n",
    "4. **Handling Variability:**\n",
    "   Images can have variations in lighting, orientation, scale, and pose. Local feature extraction can handle these variations effectively by focusing on local patterns that are relatively invariant to changes in the overall scene. This robustness makes local features useful for tasks like object recognition across different conditions.\n",
    "\n",
    "5. **Efficient and Scalable:**\n",
    "   Analyzing the entire image as a whole can be computationally expensive, especially for large images. Local feature extraction enables efficient processing by considering smaller regions of the image. This approach is scalable and allows for parallelization, which is crucial for real-time and large-scale applications.\n",
    "\n",
    "6. **Fine-Grained Analysis:**\n",
    "   For tasks like image segmentation or fine-grained classification, local feature extraction provides insights into the detailed characteristics of specific image regions. This level of granularity can be critical for distinguishing between objects of the same category that might have subtle differences.\n",
    "\n",
    "7. **Sparse Data Representation:**\n",
    "   Local features often lead to a more compact and informative representation of the image data. Instead of analyzing the entire image, local features condense the information into a more manageable and informative format, reducing computational requirements.\n",
    "\n",
    "In summary, local feature extraction provides a window into the intricate details and patterns of an image, allowing for better object recognition, localization, and contextual understanding. This approach is particularly effective for tasks that involve analyzing specific regions within images and for handling variations, scalability, and complexity inherent in real-world visual data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821dd647-8863-43ee-badd-6e1418689f77",
   "metadata": {},
   "source": [
    "## Ì§8. Importance of Convolution and Max Pooling:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d1745-5586-451e-bf87-e20140f4a11e",
   "metadata": {},
   "source": [
    "### a. Elaborate on the importance of convolution and max pooling operations in a Convolutional Neural Network (CNN). Explain how these operations contribute to feature extraction and spatial down-sampling in CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5364f-2acc-4a8c-93f2-c8d44a96c9f7",
   "metadata": {},
   "source": [
    "Ans--> Convolution and max pooling are fundamental operations in Convolutional Neural Networks (CNNs) that play a crucial role in feature extraction and spatial down-sampling. These operations contribute significantly to the ability of CNNs to learn hierarchical features and capture spatial hierarchies within images.\n",
    "\n",
    "**Convolution Operation:**\n",
    "Convolution involves sliding a small filter (also known as a kernel) over the input image to perform element-wise multiplication and summation. The filter's values determine the weights used for multiplication at each position. Convolution is used to detect specific patterns, textures, and features in the input image. It enables the network to learn local patterns and extract meaningful information from different parts of the image.\n",
    "\n",
    "Advantages of Convolution:\n",
    "- **Feature Extraction:** Convolution allows the network to identify edges, corners, and textures in various orientations and scales.\n",
    "- **Hierarchical Feature Learning:** By using multiple convolutional layers with different filters, CNNs can learn hierarchical features that capture increasingly complex patterns.\n",
    "\n",
    "**Max Pooling Operation:**\n",
    "Max pooling is a down-sampling operation that reduces the spatial dimensions of the feature maps while retaining important information. In max pooling, a small window (usually 2x2 or 3x3) slides over the feature map, and the maximum value within each window is selected as the representative value for that region. Max pooling achieves spatial down-sampling by focusing on the most salient features while discarding less significant details.\n",
    "\n",
    "Advantages of Max Pooling:\n",
    "- **Spatial Down-Sampling:** Max pooling reduces the spatial dimensions of feature maps, making subsequent layers computationally more efficient and reducing the number of parameters.\n",
    "- **Translation Invariance:** By selecting the maximum value within each window, max pooling introduces a degree of translation invariance. This means the network can recognize patterns even if they're slightly shifted in the input image.\n",
    "- **Robustness to Noise:** Max pooling can help suppress small variations or noise in the input, leading to a more robust representation of features.\n",
    "\n",
    "How Convolution and Max Pooling Contribute:\n",
    "1. **Feature Extraction:** Convolution operations scan the image to detect local features like edges, textures, and patterns.\n",
    "2. **Hierarchical Learning:** Multiple convolutional layers learn progressively more complex and abstract features.\n",
    "3. **Spatial Down-Sampling:** Max pooling reduces the spatial dimensions, making the network more computationally efficient.\n",
    "4. **Feature Localization:** The features detected by convolution are preserved as spatial information is down-sampled by max pooling.\n",
    "5. **Reduced Overfitting:** Max pooling reduces the chance of overfitting by discarding minor variations and focusing on salient features.\n",
    "\n",
    "In summary, convolution and max pooling are key operations in CNNs that enable the network to capture hierarchical features, down-sample spatial dimensions, and create robust and efficient representations of images. These operations play a vital role in the success of CNNs for various computer vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b45810-52a2-44a7-b8ab-70de551aea2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
